{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.stats import norm, binomtest\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "\n",
    "class Smooth(object):\n",
    "    \"\"\"A smoothed classifier g \"\"\"\n",
    "\n",
    "    # to abstain, Smooth returns this int\n",
    "    ABSTAIN = -1\n",
    "\n",
    "    def __init__(self, base_classifier: torch.nn.Module, num_classes: int, sigma: float):\n",
    "        \"\"\"\n",
    "        :param base_classifier: maps from [batch x channel x height x width] to [batch x num_classes]\n",
    "        :param num_classes:\n",
    "        :param sigma: the noise level hyperparameter\n",
    "        \"\"\"\n",
    "        self.base_classifier = base_classifier\n",
    "        self.num_classes = num_classes\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def certify(self, x: torch.tensor, n0: int, n: int, alpha: float, batch_size: int) -> (int, float):\n",
    "        \"\"\" Monte Carlo algorithm for certifying that g's prediction around x is constant within some L2 radius.\n",
    "        With probability at least 1 - alpha, the class returned by this method will equal g(x), and g's prediction will\n",
    "        robust within a L2 ball of radius R around x.\n",
    "\n",
    "        :param x: the input [channel x height x width]\n",
    "        :param n0: the number of Monte Carlo samples to use for selection\n",
    "        :param n: the number of Monte Carlo samples to use for estimation\n",
    "        :param alpha: the failure probability\n",
    "        :param batch_size: batch size to use when evaluating the base classifier\n",
    "        :return: (predicted class, certified radius)\n",
    "                 in the case of abstention, the class will be ABSTAIN and the radius 0.\n",
    "        \"\"\"\n",
    "        self.base_classifier.eval()\n",
    "        # draw samples of f(x+ epsilon)\n",
    "        counts_selection = self._sample_noise(x, n0, batch_size)\n",
    "        # use these samples to take a guess at the top class\n",
    "        cAHat = counts_selection.argmax().item()\n",
    "        # draw more samples of f(x + epsilon)\n",
    "        counts_estimation = self._sample_noise(x, n, batch_size)\n",
    "        # use these samples to estimate a lower bound on pA\n",
    "        nA = counts_estimation[cAHat].item()\n",
    "        pABar = self._lower_confidence_bound(nA, n, alpha)\n",
    "        if pABar < 0.5:\n",
    "            return Smooth.ABSTAIN, 0.0\n",
    "        else:\n",
    "            radius = self.sigma * norm.ppf(pABar)\n",
    "            return cAHat, radius\n",
    "\n",
    "    def predict(self, x: torch.tensor, n: int, alpha: float, batch_size: int) -> int:\n",
    "        \"\"\" Monte Carlo algorithm for evaluating the prediction of g at x.  With probability at least 1 - alpha, the\n",
    "        class returned by this method will equal g(x).\n",
    "\n",
    "        This function uses the hypothesis test described in https://arxiv.org/abs/1610.03944\n",
    "        for identifying the top category of a multinomial distribution.\n",
    "\n",
    "        :param x: the input [channel x height x width]\n",
    "        :param n: the number of Monte Carlo samples to use\n",
    "        :param alpha: the failure probability\n",
    "        :param batch_size: batch size to use when evaluating the base classifier\n",
    "        :return: the predicted class, or ABSTAIN\n",
    "        \"\"\"\n",
    "        self.base_classifier.eval()\n",
    "        counts = self._sample_noise(x, n, batch_size)\n",
    "        top2 = counts.argsort()[::-1][:2]\n",
    "        count1 = counts[top2[0]]\n",
    "        count2 = counts[top2[1]]\n",
    "        if binomtest(count1, count1 + count2, p=0.5) > alpha:\n",
    "            return Smooth.ABSTAIN\n",
    "        else:\n",
    "            return top2[0]\n",
    "\n",
    "    def _sample_noise(self, x: torch.tensor, num: int, batch_size) -> np.ndarray:\n",
    "        \"\"\" Sample the base classifier's prediction under noisy corruptions of the input x.\n",
    "\n",
    "        :param x: the input [channel x width x height]\n",
    "        :param num: number of samples to collect\n",
    "        :param batch_size:\n",
    "        :return: an ndarray[int] of length num_classes containing the per-class counts\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            counts = np.zeros(self.num_classes, dtype=int)\n",
    "            for _ in range(ceil(num / batch_size)):\n",
    "                \n",
    "                # print(_)\n",
    "\n",
    "                this_batch_size = min(batch_size, num)\n",
    "                num -= this_batch_size\n",
    "\n",
    "                batch = x.repeat((this_batch_size, 1, 1, 1))\n",
    "                noise = torch.randn_like(batch, device='cuda') * self.sigma\n",
    "                predictions = self.base_classifier(batch + noise).argmax(1)\n",
    "                counts += self._count_arr(predictions.cpu().numpy(), self.num_classes)\n",
    "            return counts\n",
    "\n",
    "    def _count_arr(self, arr: np.ndarray, length: int) -> np.ndarray:\n",
    "        counts = np.zeros(length, dtype=int)\n",
    "        for idx in arr:\n",
    "            counts[idx] += 1\n",
    "        return counts\n",
    "\n",
    "    def _lower_confidence_bound(self, NA: int, N: int, alpha: float) -> float:\n",
    "        \"\"\" Returns a (1 - alpha) lower confidence bound on a bernoulli proportion.\n",
    "\n",
    "        This function uses the Clopper-Pearson method.\n",
    "\n",
    "        :param NA: the number of \"successes\"\n",
    "        :param N: the number of total draws\n",
    "        :param alpha: the confidence level\n",
    "        :return: a lower bound on the binomial proportion which holds true w.p at least (1 - alpha) over the samples\n",
    "        \"\"\"\n",
    "        return proportion_confint(NA, N, alpha=2 * alpha, method=\"beta\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "the size of test set is (1000, 32, 32, 3)\n",
      "the size of test label is (1000,)\n"
     ]
    }
   ],
   "source": [
    "# evaluate a smoothed classifier on a dataset\n",
    "import os\n",
    "import setGPU\n",
    "from time import time\n",
    "import torch\n",
    "import datetime\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from iCIFAR100 import iCIFAR100\n",
    "from myNetwork import network\n",
    "from ResNet import resnet18_cbam\n",
    "\n",
    "\n",
    "classes=[5, 15]\n",
    "sigma = 0.25\n",
    "outfile = \"certify/ca_model2_0514.txt\"\n",
    "batch = 100\n",
    "skip = 1\n",
    "max = -1\n",
    "N0 = 100\n",
    "N = 1000\n",
    "alpha = 0.001\n",
    "\n",
    "feature_extractor=resnet18_cbam()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    numclass=20\n",
    "\n",
    "    # load the base classifier\n",
    "    # base_classifier = network(numclass,feature_extractor).to(device)\n",
    "    base_classifier = torch.load('model/accuracy_73.900_KNN_accuracy_76.050_increment_29_net.pt').to(device)\n",
    "\n",
    "    # load the dataset\n",
    "    test_transform = transforms.Compose([#transforms.Resize(img_size),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
    "    dataset = iCIFAR100('dataset', train=False, transform=test_transform, download=True)\n",
    "    dataset.getTestData(classes)\n",
    "    dataset = DataLoader(dataset=dataset,\n",
    "                                 shuffle=True,\n",
    "                                 batch_size=1)\n",
    "\n",
    "    # create the smooothed classifier g\n",
    "    smoothed_classifier = Smooth(base_classifier, numclass, sigma)\n",
    "\n",
    "    # prepare output file\n",
    "    f = open(outfile, 'w')\n",
    "    print(\"idx\\tlabel\\tpredict\\tradius\\tcorrect\\ttime\", file=f, flush=True)\n",
    "    res = []\n",
    "\n",
    "    # iterate through the dataset\n",
    "    for i, (indexs, imgs, labels) in enumerate(dataset):\n",
    "\n",
    "        # only certify every args.skip examples, and stop after args.max examples\n",
    "        if i % skip != 0:\n",
    "            continue\n",
    "        if i == max:\n",
    "            break\n",
    "\n",
    "        (x, label) = (imgs, labels)\n",
    "\n",
    "        before_time = time()\n",
    "        # certify the prediction of g around x\n",
    "        x = x.cuda()\n",
    "        prediction, radius = smoothed_classifier.certify(x, N0, N, alpha, batch)\n",
    "        after_time = time()\n",
    "        correct = int(prediction == label)\n",
    "\n",
    "        res.append([prediction, label, radius])\n",
    "\n",
    "        time_elapsed = str(datetime.timedelta(seconds=(after_time - before_time)))\n",
    "        print(\"{}\\t{}\\t{}\\t{:.3}\\t{}\\t{}\".format(\n",
    "            i, label, prediction, radius, correct, time_elapsed), file=f, flush=True)\n",
    "\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred, label, radius \u001b[38;5;129;01min\u001b[39;00m \u001b[43mres\u001b[49m:\n\u001b[0;32m      4\u001b[0m     accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m==\u001b[39m label \u001b[38;5;129;01mand\u001b[39;00m radius \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m)\n\u001b[0;32m      5\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "total = 0\n",
    "for pred, label, radius in res:\n",
    "    accuracy += (pred == label and radius >= 0.25)\n",
    "    total += 1\n",
    "\n",
    "print(accuracy)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smooth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
